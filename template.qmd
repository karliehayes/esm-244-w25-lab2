---
title: "lab-2"
format: 
  html:
    code-folding: show
    embed-resources: true
execute:
  warning: false
  message: false
---


```{r}
# load libraries
library(tidyverse)
library(palmerpenguins)
```

What does the following code chunk do? Why do we want to do these steps?

```{r}
# cleaning up the names - making the variable names shorter (just so we don't have to type a lot)
penguins_clean<-penguins |> 
  drop_na() |> 
  rename(mass=body_mass_g,
         bill_l=bill_length_mm,
         bill_d=bill_depth_mm,
         flip_l=flipper_length_mm)
```


## Part 1: Set up models

We are tasked with providing a penguin growth model to support conservation efforts in Antartica. The lead researcher needs an accurate, but parsimonious model to predict penguin body mass based on observed characteristics. They asked us to analyze 3 models:

- Model 1: Bill length, bill depth, flipper length, species, sex, and island

- Model 2: Bill length, bill depth, flipper length, species, and sex

- Model 3: Bill depth, flipper length, species, and sex

Run a linear model for each model specification. Summarize your findings. Use the `penguins_clean` dataframe.

**New Feature!**

R is able to recognize formulas if saved to the global environment. Take advantage of that using the following code chunk as inspiration:

```{r}
#| eval: false

#variable name
# purpose -- tidies up the equation
f1   <-  dep_var~col_name_1+col_name_2+col_name_3

mdl<-lm(f1, data=df_where_column_names_come_frome)
```
- Model 1: Bill length, bill depth, flipper length, species, sex, and island

- Model 2: Bill length, bill depth, flipper length, species, and sex

- Model 3: Bill depth, flipper length, species, and sex

```{r}
# Use this chunk and any extras to complete setting up the model
f1 <- mass~bill_d + flip_l + species + sex + bill_l + island
f2 <- mass~bill_d + flip_l + species + sex + bill_l
f3 <- mass~bill_d + flip_l + species + sex

model_1 <- lm(f1, data = penguins_clean)

model_2 <- lm(f2, data = penguins_clean)

model_3 <- lm(f3, data = penguins_clean)

summary(model_1)
# all variables except island are significant
# adj R2 : .8721 --> explains 87% of the variation in the data -- good model
summary(model_2)
# all variables 
# adj R2 : .8727 --> explains 87% of the variation in the data -- good model
summary(model_3)
# findings 
# adj R2 : .8705 --> explains 87% of the variation in the data -- good model



```



### AIC

Use AIC to justify your model selection. What edits do you need to make in order for the chunk below to work? Interpret the output. *Bonus:* Try to make the rendered output pretty by putting it into a table.

```{r}
#| eval: false

AIC(model_1, model_2, model_3)
# model 2: lowest AIC value - this would suggest that model 2 is the best
```


## Comparing models with Cross Validation

Now we're going to use 10-fold cross validation to help us select the models. Write out some pseudocode to outline the process we'll need to implement.

Pseudocode:
take the penguins_clean data set 
divide it into a training and test set (use random selection)
what metric 
  Root mean squared error
  make a function for RMSE

for loop 
  apply the model to each training set (aka fold)
  make predictions on test set with the fitted training model
close loop

Summarize our RMSE + compare the RMSE scores
final model built on whole dataset


### Accuracy criteria

What metric is going to help us identify which model performed better?

[Here's an extensive list of options](https://www.geeksforgeeks.org/metrics-for-machine-learning-model/#)

We'll use root mean square error to start as it is the most widely used.

What is the difference between these two functions? Create two example vectors `x` and `y` to test each. Make sure to run the functions before trying them out. Which do you prefer using?

```{r}
x = c(1,3,4,2,5,4,3,2,5)
y = c(8,7,6,8,5,6,7,8,7)

calc_rmse<-function(x,y){
  rmse <- (x-y)^2 |> 
    mean() |> 
    sqrt()
  return(rmse)
}

calc_rmse_2<-function(x,y){
  rmse<- sqrt(mean((x-y)^2))
  
  return(rmse)
}

calc_rmse(x, y)
calc_rmse_2(x, y)
```


### Testing and Training split

We need to randomly assign every data point to a fold. We're going to want 10 folds. 

**New Function!**

`sample()` takes a random draw from a vector we pass into it. For example, we can tell sample to extract a random value from a vector of 1 through 5

```{r}
ex<-seq(1,5)
sample(ex,size=1)

# we can create a random sample of any size with the size term.

# Why doesn't the first line work while the second does?
sample(ex,size=10)
sample(ex,size=10,replace=TRUE)

#Describe in words the replace argument.

# you replace the sample that you take out back into the original data set when you take another sample 

```

Why is everybody getting different answers in the example sample? Is this a problem for reproducible datascience and will it affect our results (Like would Nathan have different model results than Yutian?)

```{r}
#seed
set.seed(42)
sample(ex, size = 10, replace= TRUE)

```


Now let's use sample in tidyverse structure to group the data into different folds.

```{r}
folds<-10

# assigning all the penguins to a group from 1 to 10
fold_vec<-rep(1:folds,length.out=nrow(penguins_clean))
fold_vec

# create a new column where the fold number is assigned 
# now we know all of the folds that the data should go in'
penguins_fold<-penguins_clean |> 
  mutate(group=sample(fold_vec, size = n(), replace = FALSE))
  

#check to make sure the fold groups are balanced

table(penguins_fold$group)
```

Create dataframes called `test_df` and `train_df` that split the penguins data into a train or test sample

```{r}
# datasets here
test_df <- penguins_fold |>
  filter(group == 1)

train_df <- penguins_fold |>
  filter(group != 1)
```


Now fit each model to the training set using the `lm()`. Name each model `training_lmX` where X is the number of the formula.

```{r}

training_lm1 <- lm(f1, data = train_df)

training_lm2 <- lm(f2, data = train_df)

training_lm3 <- lm(f3, data = train_df)

```



**New Function!**

`predict()` uses R models to run predictions with new data. In our case the structure would look something like what we see below. What do I need to do to make this chunk work?

```{r}
# adds the predicted data as a new column in the existing data set
predict_test<-test_df |> 
  mutate(model1 = predict(training_lm1,test_df),
         model2 = predict(training_lm2,test_df),
         model3 = predict(training_lm3,test_df))
```

Calculate the RMSE of the first fold test predictions. Hint: Use summarize to condense the `predict_test` dataframe.

```{r}

rmse_predict_test<-predict_test |>
  summarize(calc_rmse(mass,model1),
            calc_rmse(mass,model2),
            calc_rmse(mass,model3))

rmse_predict_test
```

What are the results just looking at the first fold?

### 10-fold CV: For Loop

Our general structure works with one fold. Now we need to evaluate across all 10 folds for each model. Let's use a for loop to iterate over the folds for just one model first.

```{r}

### initialize a blank vector
rmse_vec<-vector(length=folds)  
# blank vector - we are going to fill in the vector using the for loop
#Why? it's good to make a vector of the length that you want to iterate 
# do this before you run your for loop so R doesn't have to keep making vectors

# 1:folds is 1:10
for( i in 1:folds){

  # separate into test and train
  kfold_test_df <- penguins_fold |>
    filter(group == i)
  
  kfold_train_df <- penguins_fold |>
    filter(group != i)
  
  # Run for one model
  k_training_lm <- lm(f1, data = kfold_train_df)
  
  #Get the predictions from the model
  predict_test<-kfold_test_df |> 
    mutate(model1 = predict(k_training_lm,kfold_test_df))
  
  # Summarize/calculate the rmse for that model
  kfold_rmse<- predict_test |>
    summarize(rmse_md1=calc_rmse(mass,model1))
  
  rmse_vec[i]<-kfold_rmse$rmse_md1 # go into this position in the vector and add in the rmse value
  # review this
}

# Average value for the first model
rmse_vec
mean(rmse_vec)
```


Great we made a for loop for one model. Now we would have to do it again and again for the other formulas. To reduce copy/pasting let's make the innerpart of the for loop into a function. I gave you the starting pieces below. Complete the rest of the function

```{r}
kfold_cv<-function(i,df,formula){
  
  # split into training and test
  kfold_train_df <- df |>
    filter(group != i)
  
  kfold_test_df <- df |>
    filter(group == i)
  
  # run model
  kfold_lm <- lm(formula, data = kfold_train_df)
  
  # get prediction
  kfold_pred_df <- kfold_test_df |>
    mutate(md = predict(kfold_lm, kfold_test_df))
  
  # calculate rmse
  kfold_rmse <- kfold_pred_df |>
    summarize(rmse_md = calc_rmse(md, mass))
  
  return(kfold_rmse$rmse_md)
  
  
}
```





### 10-fold CV: Purrr

Since we already defined the function that does CV for each model. We can use purrr to easily get all the results and store it in a dataframe.

```{r}

# purpose of using purrr: you can pipe the df in 
rmse_df<-data.frame(j=1:folds) |> mutate(rmse_mdl1 = map_dbl(j, kfold_cv, df=penguins_fold,formula=f1),
                                         rmse_mdl2=map_dbl(j,kfold_cv,df=penguins_fold,formula=f2),
                                         rmse_mdl3=map_dbl(j,kfold_cv,df=penguins_fold,formula=f3))

rmse_means<-rmse_df |> 
  summarize(across(starts_with('rmse'),mean)) # summarize means of all of the columns that start with rmse
# model 2 is the best 
```


## Final Model Selection

Between AIC and the RMSE scores of the Cross Validation, which model does the best job of predicting penguin bodymass?
Model 2

The final step is to run the selected model on all the data. Fit a final model and provide the summary table.
```{r}

finalmodel<- lm(f2, data = penguins_clean)

summary(finalmodel)

```


Render your document, commit changes, and push to github.

